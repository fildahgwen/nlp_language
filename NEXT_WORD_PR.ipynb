{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq4KZWrnxndP"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file = open(\"/content/drive/MyDrive/SHONA.txt\",\"r\")\n",
        "shonads = file.read()\n",
        "#!wget -O SHONA.txt \"https://drive.google.com/file/d/1BMSyxt2aK2u_MgQ6cFvPnCTvOIzDISXK/view?usp=sharing\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOkUyUby1imQ",
        "outputId": "28b09cea-b2d0-4e2c-9e84-b6040699472e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shonads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "aICEHe1wNY56",
        "outputId": "95837485-29c9-4961-a630-6e536b5bc9e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffPAZVIRONGWA zvapfuura takatarisa mamiriro erondedzero kubva kunhanganyaya kusvika kumhedziso. Rwendo rwuno tinoda kuchiongorora mhando dzacho dzerondedzero. Tinoda kubata mhando imwe neimwe tichipanana mabatiro atingadziita mubvunzo.Tine rondedzero inonyanyozivikanwa nevakawanda. Iyi ndeye tsanangudzo.\\n\\n\\nVana vazhinji vanokwanisa chose kunyora rondedzero yetsanangudzo nekuti haina nhongwa dzakawanda. Iyi rondedzero hainetsi kunyora nekuti unenge uchitaura pamusoro pechinhu chawakwanisa kuona nemaziso ako.\\n\\n\\nUnogona zvakare kunge uchichibata namawoko ako nekudaro haupererwi nezvekunyora.\\n\\n\\nIyi rondedzero hainyanyoda zidengu rezvirungamutauro kunze kwefananidzo. Pasi pemhando iyi ndipo panopinda misoro inoti, \"Chipfuyo chandinofarira, Baba vangu, Mudzidzisi wangu kana Chikoro chedu\".Chinongodiwa apa kuunganidza pfungwa\\n\\n\\ndzako wodzironga zvichibva kumusoro kudzika kuzasi (top to bottom description) kana kuti kubva kuzasi uchikwidza (bottom to top approach).\\n\\n\\nKunouyawo rondedzero yenhoroondo. Iyi inodana humhizha nehutsanzi hwemwana hwekurudunura mutauro wakarungwa apo anenge apihwa mukana wekupa nhoroondo yezvakaitika muupenyu. Inopa mwana mukana wekutaura chero zvaasina kuona asi achingoshandisa huchenjeri hwake.\\n\\n\\nChakakosha kubata dingindira remusoro wenyaya nekuti unogona kufuririka kukanda zvirungamutauro iwe wabuda kare mugwara.\\n\\n\\nNdipo panopinda rondedzero dzine misoro inoti, \"Zuva randisingakanganwi\", \"Mabiko andakaenda\" kana \"Rwendo rwandakafamba nebhazi.\"\\n\\n\\nKune dzimwe nguva dzinonyorwa rondedzero dzine nechekuita nekukakavara. Rondedzero yegakavaZvino kuti ukwezve pfungwa dzemukwenyi, unotoda kuti ubate kwese (mativi ose) uchiburitsa huipi nehunaku hwacho. Munyori anozokarirwawo kuti azowana divi rake rimwe chete raanozoti naro kurerekera.\\n\\n\\nPanogona kubuda musoro wenyaya unoti, \"Kugara kumaruwa kwakanaka kupfuura mumadhorobha\".\\n\\n\\nChinofanirwa kungwarirwa apa ndechekuti unofanira kuzoburitsa kuti kumaruwa kwakanaka pekupedzisira nekuti ndizvo zviri kuda musoro wenyaya.\\n\\n\\nAsi dai musoro wanga wakati, \"Ndekupi kwakanaka kugara kumaruwa kana kudhorobha\" waizogona kungosarudza divi rako raunoramba wakarerekera uchipa tsigiro.\\n\\n\\nIyi mhando inonyanyoda vana vaya vava kuchikoro chepamusoro.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2Id-uuNKVbAz"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gnocea6Vn1R",
        "outputId": "1de832f2-5a08-4406-a05a-02802c818ae2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXmHecuiV0Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def clean_text(shonads):\n",
        "    # Convert text to lowercase\n",
        "    shonads = shonads.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    shonads = re.sub(r'[^\\w\\s]', '', shonads)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(shonads)\n",
        "\n",
        "    # Join the words back into a cleaned text\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text(shonads)"
      ],
      "metadata": {
        "id": "wR93kIMbrOkl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "dYyZNTqlV0Sc",
        "outputId": "05e327e8-56ce-4866-f4ac-22cdbb541270"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pazvirongwa zvapfuura takatarisa mamiriro erondedzero kubva kunhanganyaya kusvika kumhedziso rwendo rwuno tinoda kuchiongorora mhando dzacho dzerondedzero tinoda kubata mhando imwe neimwe tichipanana mabatiro atingadziita mubvunzotine rondedzero inonyanyozivikanwa nevakawanda iyi ndeye tsanangudzo vana vazhinji vanokwanisa chose kunyora rondedzero yetsanangudzo nekuti haina nhongwa dzakawanda iyi rondedzero hainetsi kunyora nekuti unenge uchitaura pamusoro pechinhu chawakwanisa kuona nemaziso ako unogona zvakare kunge uchichibata namawoko ako nekudaro haupererwi nezvekunyora iyi rondedzero hainyanyoda zidengu rezvirungamutauro kunze kwefananidzo pasi pemhando iyi ndipo panopinda misoro inoti chipfuyo chandinofarira baba vangu mudzidzisi wangu kana chikoro cheduchinongodiwa apa kuunganidza pfungwa dzako wodzironga zvichibva kumusoro kudzika kuzasi top to bottom description kana kuti kubva kuzasi uchikwidza bottom to top approach kunouyawo rondedzero yenhoroondo iyi inodana humhizha nehutsanzi hwemwana hwekurudunura mutauro wakarungwa apo anenge apihwa mukana wekupa nhoroondo yezvakaitika muupenyu inopa mwana mukana wekutaura chero zvaasina kuona asi achingoshandisa huchenjeri hwake chakakosha kubata dingindira remusoro wenyaya nekuti unogona kufuririka kukanda zvirungamutauro iwe wabuda kare mugwara ndipo panopinda rondedzero dzine misoro inoti zuva randisingakanganwi mabiko andakaenda kana rwendo rwandakafamba nebhazi kune dzimwe nguva dzinonyorwa rondedzero dzine nechekuita nekukakavara rondedzero yegakavazvino kuti ukwezve pfungwa dzemukwenyi unotoda kuti ubate kwese mativi ose uchiburitsa huipi nehunaku hwacho munyori anozokarirwawo kuti azowana divi rake rimwe chete raanozoti naro kurerekera panogona kubuda musoro wenyaya unoti kugara kumaruwa kwakanaka kupfuura mumadhorobha chinofanirwa kungwarirwa apa ndechekuti unofanira kuzoburitsa kuti kumaruwa kwakanaka pekupedzisira nekuti ndizvo zviri kuda musoro wenyaya asi dai musoro wanga wakati ndekupi kwakanaka kugara kumaruwa kana kudhorobha waizogona kungosarudza divi rako raunoramba wakarerekera uchipa tsigiro iyi mhando inonyanyoda vana vaya vava kuchikoro chepamusoro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(cleaned_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNCtLPvfXEm9",
        "outputId": "13db301f-4546-4ba1-f3ed-3988775dbbfd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pazvirongwa', 'zvapfuura', 'takatarisa', 'mamiriro', 'erondedzero', 'kubva', 'kunhanganyaya', 'kusvika', 'kumhedziso', 'rwendo', 'rwuno', 'tinoda', 'kuchiongorora', 'mhando', 'dzacho', 'dzerondedzero', 'tinoda', 'kubata', 'mhando', 'imwe', 'neimwe', 'tichipanana', 'mabatiro', 'atingadziita', 'mubvunzotine', 'rondedzero', 'inonyanyozivikanwa', 'nevakawanda', 'iyi', 'ndeye', 'tsanangudzo', 'vana', 'vazhinji', 'vanokwanisa', 'chose', 'kunyora', 'rondedzero', 'yetsanangudzo', 'nekuti', 'haina', 'nhongwa', 'dzakawanda', 'iyi', 'rondedzero', 'hainetsi', 'kunyora', 'nekuti', 'unenge', 'uchitaura', 'pamusoro', 'pechinhu', 'chawakwanisa', 'kuona', 'nemaziso', 'ako', 'unogona', 'zvakare', 'kunge', 'uchichibata', 'namawoko', 'ako', 'nekudaro', 'haupererwi', 'nezvekunyora', 'iyi', 'rondedzero', 'hainyanyoda', 'zidengu', 'rezvirungamutauro', 'kunze', 'kwefananidzo', 'pasi', 'pemhando', 'iyi', 'ndipo', 'panopinda', 'misoro', 'inoti', 'chipfuyo', 'chandinofarira', 'baba', 'vangu', 'mudzidzisi', 'wangu', 'kana', 'chikoro', 'cheduchinongodiwa', 'apa', 'kuunganidza', 'pfungwa', 'dzako', 'wodzironga', 'zvichibva', 'kumusoro', 'kudzika', 'kuzasi', 'top', 'to', 'bottom', 'description', 'kana', 'kuti', 'kubva', 'kuzasi', 'uchikwidza', 'bottom', 'to', 'top', 'approach', 'kunouyawo', 'rondedzero', 'yenhoroondo', 'iyi', 'inodana', 'humhizha', 'nehutsanzi', 'hwemwana', 'hwekurudunura', 'mutauro', 'wakarungwa', 'apo', 'anenge', 'apihwa', 'mukana', 'wekupa', 'nhoroondo', 'yezvakaitika', 'muupenyu', 'inopa', 'mwana', 'mukana', 'wekutaura', 'chero', 'zvaasina', 'kuona', 'asi', 'achingoshandisa', 'huchenjeri', 'hwake', 'chakakosha', 'kubata', 'dingindira', 'remusoro', 'wenyaya', 'nekuti', 'unogona', 'kufuririka', 'kukanda', 'zvirungamutauro', 'iwe', 'wabuda', 'kare', 'mugwara', 'ndipo', 'panopinda', 'rondedzero', 'dzine', 'misoro', 'inoti', 'zuva', 'randisingakanganwi', 'mabiko', 'andakaenda', 'kana', 'rwendo', 'rwandakafamba', 'nebhazi', 'kune', 'dzimwe', 'nguva', 'dzinonyorwa', 'rondedzero', 'dzine', 'nechekuita', 'nekukakavara', 'rondedzero', 'yegakavazvino', 'kuti', 'ukwezve', 'pfungwa', 'dzemukwenyi', 'unotoda', 'kuti', 'ubate', 'kwese', 'mativi', 'ose', 'uchiburitsa', 'huipi', 'nehunaku', 'hwacho', 'munyori', 'anozokarirwawo', 'kuti', 'azowana', 'divi', 'rake', 'rimwe', 'chete', 'raanozoti', 'naro', 'kurerekera', 'panogona', 'kubuda', 'musoro', 'wenyaya', 'unoti', 'kugara', 'kumaruwa', 'kwakanaka', 'kupfuura', 'mumadhorobha', 'chinofanirwa', 'kungwarirwa', 'apa', 'ndechekuti', 'unofanira', 'kuzoburitsa', 'kuti', 'kumaruwa', 'kwakanaka', 'pekupedzisira', 'nekuti', 'ndizvo', 'zviri', 'kuda', 'musoro', 'wenyaya', 'asi', 'dai', 'musoro', 'wanga', 'wakati', 'ndekupi', 'kwakanaka', 'kugara', 'kumaruwa', 'kana', 'kudhorobha', 'waizogona', 'kungosarudza', 'divi', 'rako', 'raunoramba', 'wakarerekera', 'uchipa', 'tsigiro', 'iyi', 'mhando', 'inonyanyoda', 'vana', 'vaya', 'vava', 'kuchikoro', 'chepamusoro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#saving the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "7K3XKKMeXNgC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a vocab"
      ],
      "metadata": {
        "id": "tQAjjzNKXiCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#build an optimal vocabulary\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "max_vocab_size= 10000  # the vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocabulary_size = min(max_vocab_size, len(word_index))"
      ],
      "metadata": {
        "id": "STplfAeIXNku"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ39IZLagaQH",
        "outputId": "b8f98e52-0ab9-41d1-842b-c76c6ab39029"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rondedzero': 1,\n",
              " 'iyi': 2,\n",
              " 'kuti': 3,\n",
              " 'nekuti': 4,\n",
              " 'kana': 5,\n",
              " 'mhando': 6,\n",
              " 'wenyaya': 7,\n",
              " 'musoro': 8,\n",
              " 'kumaruwa': 9,\n",
              " 'kwakanaka': 10,\n",
              " 'kubva': 11,\n",
              " 'rwendo': 12,\n",
              " 'tinoda': 13,\n",
              " 'kubata': 14,\n",
              " 'vana': 15,\n",
              " 'kunyora': 16,\n",
              " 'kuona': 17,\n",
              " 'ako': 18,\n",
              " 'unogona': 19,\n",
              " 'ndipo': 20,\n",
              " 'panopinda': 21,\n",
              " 'misoro': 22,\n",
              " 'inoti': 23,\n",
              " 'apa': 24,\n",
              " 'pfungwa': 25,\n",
              " 'kuzasi': 26,\n",
              " 'top': 27,\n",
              " 'to': 28,\n",
              " 'bottom': 29,\n",
              " 'mukana': 30,\n",
              " 'asi': 31,\n",
              " 'dzine': 32,\n",
              " 'divi': 33,\n",
              " 'kugara': 34,\n",
              " 'pazvirongwa': 35,\n",
              " 'zvapfuura': 36,\n",
              " 'takatarisa': 37,\n",
              " 'mamiriro': 38,\n",
              " 'erondedzero': 39,\n",
              " 'kunhanganyaya': 40,\n",
              " 'kusvika': 41,\n",
              " 'kumhedziso': 42,\n",
              " 'rwuno': 43,\n",
              " 'kuchiongorora': 44,\n",
              " 'dzacho': 45,\n",
              " 'dzerondedzero': 46,\n",
              " 'imwe': 47,\n",
              " 'neimwe': 48,\n",
              " 'tichipanana': 49,\n",
              " 'mabatiro': 50,\n",
              " 'atingadziita': 51,\n",
              " 'mubvunzotine': 52,\n",
              " 'inonyanyozivikanwa': 53,\n",
              " 'nevakawanda': 54,\n",
              " 'ndeye': 55,\n",
              " 'tsanangudzo': 56,\n",
              " 'vazhinji': 57,\n",
              " 'vanokwanisa': 58,\n",
              " 'chose': 59,\n",
              " 'yetsanangudzo': 60,\n",
              " 'haina': 61,\n",
              " 'nhongwa': 62,\n",
              " 'dzakawanda': 63,\n",
              " 'hainetsi': 64,\n",
              " 'unenge': 65,\n",
              " 'uchitaura': 66,\n",
              " 'pamusoro': 67,\n",
              " 'pechinhu': 68,\n",
              " 'chawakwanisa': 69,\n",
              " 'nemaziso': 70,\n",
              " 'zvakare': 71,\n",
              " 'kunge': 72,\n",
              " 'uchichibata': 73,\n",
              " 'namawoko': 74,\n",
              " 'nekudaro': 75,\n",
              " 'haupererwi': 76,\n",
              " 'nezvekunyora': 77,\n",
              " 'hainyanyoda': 78,\n",
              " 'zidengu': 79,\n",
              " 'rezvirungamutauro': 80,\n",
              " 'kunze': 81,\n",
              " 'kwefananidzo': 82,\n",
              " 'pasi': 83,\n",
              " 'pemhando': 84,\n",
              " 'chipfuyo': 85,\n",
              " 'chandinofarira': 86,\n",
              " 'baba': 87,\n",
              " 'vangu': 88,\n",
              " 'mudzidzisi': 89,\n",
              " 'wangu': 90,\n",
              " 'chikoro': 91,\n",
              " 'cheduchinongodiwa': 92,\n",
              " 'kuunganidza': 93,\n",
              " 'dzako': 94,\n",
              " 'wodzironga': 95,\n",
              " 'zvichibva': 96,\n",
              " 'kumusoro': 97,\n",
              " 'kudzika': 98,\n",
              " 'description': 99,\n",
              " 'uchikwidza': 100,\n",
              " 'approach': 101,\n",
              " 'kunouyawo': 102,\n",
              " 'yenhoroondo': 103,\n",
              " 'inodana': 104,\n",
              " 'humhizha': 105,\n",
              " 'nehutsanzi': 106,\n",
              " 'hwemwana': 107,\n",
              " 'hwekurudunura': 108,\n",
              " 'mutauro': 109,\n",
              " 'wakarungwa': 110,\n",
              " 'apo': 111,\n",
              " 'anenge': 112,\n",
              " 'apihwa': 113,\n",
              " 'wekupa': 114,\n",
              " 'nhoroondo': 115,\n",
              " 'yezvakaitika': 116,\n",
              " 'muupenyu': 117,\n",
              " 'inopa': 118,\n",
              " 'mwana': 119,\n",
              " 'wekutaura': 120,\n",
              " 'chero': 121,\n",
              " 'zvaasina': 122,\n",
              " 'achingoshandisa': 123,\n",
              " 'huchenjeri': 124,\n",
              " 'hwake': 125,\n",
              " 'chakakosha': 126,\n",
              " 'dingindira': 127,\n",
              " 'remusoro': 128,\n",
              " 'kufuririka': 129,\n",
              " 'kukanda': 130,\n",
              " 'zvirungamutauro': 131,\n",
              " 'iwe': 132,\n",
              " 'wabuda': 133,\n",
              " 'kare': 134,\n",
              " 'mugwara': 135,\n",
              " 'zuva': 136,\n",
              " 'randisingakanganwi': 137,\n",
              " 'mabiko': 138,\n",
              " 'andakaenda': 139,\n",
              " 'rwandakafamba': 140,\n",
              " 'nebhazi': 141,\n",
              " 'kune': 142,\n",
              " 'dzimwe': 143,\n",
              " 'nguva': 144,\n",
              " 'dzinonyorwa': 145,\n",
              " 'nechekuita': 146,\n",
              " 'nekukakavara': 147,\n",
              " 'yegakavazvino': 148,\n",
              " 'ukwezve': 149,\n",
              " 'dzemukwenyi': 150,\n",
              " 'unotoda': 151,\n",
              " 'ubate': 152,\n",
              " 'kwese': 153,\n",
              " 'mativi': 154,\n",
              " 'ose': 155,\n",
              " 'uchiburitsa': 156,\n",
              " 'huipi': 157,\n",
              " 'nehunaku': 158,\n",
              " 'hwacho': 159,\n",
              " 'munyori': 160,\n",
              " 'anozokarirwawo': 161,\n",
              " 'azowana': 162,\n",
              " 'rake': 163,\n",
              " 'rimwe': 164,\n",
              " 'chete': 165,\n",
              " 'raanozoti': 166,\n",
              " 'naro': 167,\n",
              " 'kurerekera': 168,\n",
              " 'panogona': 169,\n",
              " 'kubuda': 170,\n",
              " 'unoti': 171,\n",
              " 'kupfuura': 172,\n",
              " 'mumadhorobha': 173,\n",
              " 'chinofanirwa': 174,\n",
              " 'kungwarirwa': 175,\n",
              " 'ndechekuti': 176,\n",
              " 'unofanira': 177,\n",
              " 'kuzoburitsa': 178,\n",
              " 'pekupedzisira': 179,\n",
              " 'ndizvo': 180,\n",
              " 'zviri': 181,\n",
              " 'kuda': 182,\n",
              " 'dai': 183,\n",
              " 'wanga': 184,\n",
              " 'wakati': 185,\n",
              " 'ndekupi': 186,\n",
              " 'kudhorobha': 187,\n",
              " 'waizogona': 188,\n",
              " 'kungosarudza': 189,\n",
              " 'rako': 190,\n",
              " 'raunoramba': 191,\n",
              " 'wakarerekera': 192,\n",
              " 'uchipa': 193,\n",
              " 'tsigiro': 194,\n",
              " 'inonyanyoda': 195,\n",
              " 'vaya': 196,\n",
              " 'vava': 197,\n",
              " 'kuchikoro': 198,\n",
              " 'chepamusoro': 199}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_word_index = {}\n",
        "for word, index in word_index.items():\n",
        "    if index <= vocabulary_size:\n",
        "        reduced_word_index[word] = index\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.word_index = reduced_word_index\n",
        "tokenizer.word_index[tokenizer.oov_token] = vocabulary_size + 1\n",
        "tokenizer.num_words = vocabulary_size + 1\n",
        "vocabulary_size = len(word_index)\n",
        "print(\"Vocabulary size:\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZrX2PtXXNt4",
        "outputId": "5e7bc7db-deb7-4929-8bd2-cf6f3641fbad"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word embeddings with gensim\n"
      ],
      "metadata": {
        "id": "_XCbxSmsYpKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train word2vec model with gensim\n",
        "sentences = [sentence.split() for sentence in cleaned_text.split('.')]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"w2v_sembeddings.model\")"
      ],
      "metadata": {
        "id": "D6QBNOlVYalK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bdrnn\n"
      ],
      "metadata": {
        "id": "m_ZEDwBdZ4K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(tokenizer.num_words , 100, input_length=5))\n",
        "model1.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dense(tokenizer.num_words , activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#save the trained model\n",
        "model1.save('model1.h1')\n",
        "#print the summary of the model\n",
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-8JuWQnYarj",
        "outputId": "f2474a9d-42de-4e25-8e5b-07dbf3c4893e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5, 100)            20000     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5, 300)            301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               20200     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 501800 (1.91 MB)\n",
            "Trainable params: 501800 (1.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bidirectional rnn with pretrained"
      ],
      "metadata": {
        "id": "KuF1UpYUay7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gensim = Word2Vec.load(\"word2vec_shona_embeddings.model\")\n",
        "embedding_matrix = np.zeros((tokenizer.num_words , 100))\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(tokenizer.num_words , 100, weights=[embedding_matrix], input_length=5, trainable=False))\n",
        "model2.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(tokenizer.num_words , activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#save the trained model\n",
        "model2.save('model2.h1')\n",
        "#print summary of the model\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY7nyqQyaLyQ",
        "outputId": "a11066c1-21f2-40b7-f47d-7c53aa6140de"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 100)            20000     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 5, 300)            301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               20200     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 501800 (1.91 MB)\n",
            "Trainable params: 481800 (1.84 MB)\n",
            "Non-trainable params: 20000 (78.12 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training the 2 models"
      ],
      "metadata": {
        "id": "s49_jTyNc2u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in cleaned_text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = keras.utils.to_categorical(y, num_classes=tokenizer.num_words)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "z73T84lRczif"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.layers.rnn.base_rnn import RNN\n",
        "# Training Model 1\n",
        "RNNmod = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NOd6sHsdjnQ",
        "outputId": "f1abeff7-6c99-45e8-8d23-0367780b75ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 9s 252ms/step - loss: 5.2994 - accuracy: 0.0099 - val_loss: 5.3021 - val_accuracy: 0.0196\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 5.2878 - accuracy: 0.0394 - val_loss: 5.3085 - val_accuracy: 0.0196\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 5.2749 - accuracy: 0.0345 - val_loss: 5.3209 - val_accuracy: 0.0196\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 5.2506 - accuracy: 0.0345 - val_loss: 5.3544 - val_accuracy: 0.0196\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 5.1869 - accuracy: 0.0345 - val_loss: 5.5404 - val_accuracy: 0.0196\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 5.0389 - accuracy: 0.0345 - val_loss: 6.2620 - val_accuracy: 0.0196\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 4.8802 - accuracy: 0.0345 - val_loss: 5.9741 - val_accuracy: 0.0196\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 4.7074 - accuracy: 0.0394 - val_loss: 6.3067 - val_accuracy: 0.0196\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 4.4580 - accuracy: 0.0493 - val_loss: 6.9246 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 4.2186 - accuracy: 0.0690 - val_loss: 6.8222 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 3.9816 - accuracy: 0.1232 - val_loss: 7.2917 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 3.7297 - accuracy: 0.1232 - val_loss: 7.3495 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 3.4725 - accuracy: 0.1823 - val_loss: 7.7307 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 3.1907 - accuracy: 0.2266 - val_loss: 7.3766 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.9545 - accuracy: 0.2857 - val_loss: 7.6195 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.7413 - accuracy: 0.3547 - val_loss: 7.6516 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 2.5273 - accuracy: 0.4680 - val_loss: 7.6988 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 2.3229 - accuracy: 0.5222 - val_loss: 7.6873 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 2.1628 - accuracy: 0.5862 - val_loss: 7.8098 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 2.0041 - accuracy: 0.6601 - val_loss: 7.6998 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 1.8544 - accuracy: 0.7488 - val_loss: 8.1845 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 1.7561 - accuracy: 0.7340 - val_loss: 7.7049 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 1.6297 - accuracy: 0.7980 - val_loss: 8.1887 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 1.5217 - accuracy: 0.8571 - val_loss: 7.8817 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 1.3872 - accuracy: 0.9064 - val_loss: 8.2258 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 1.3167 - accuracy: 0.8818 - val_loss: 7.9737 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 1.2277 - accuracy: 0.9310 - val_loss: 8.3003 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.1437 - accuracy: 0.9458 - val_loss: 8.2224 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 1.0597 - accuracy: 0.9458 - val_loss: 8.4069 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.9850 - accuracy: 0.9458 - val_loss: 8.2380 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.9290 - accuracy: 0.9803 - val_loss: 8.3097 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.8589 - accuracy: 0.9606 - val_loss: 8.3329 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.8215 - accuracy: 0.9754 - val_loss: 8.4394 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7703 - accuracy: 0.9754 - val_loss: 8.4113 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.7100 - accuracy: 0.9951 - val_loss: 8.5080 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.6642 - accuracy: 0.9951 - val_loss: 8.3925 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.6399 - accuracy: 0.9951 - val_loss: 8.5076 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.5940 - accuracy: 0.9901 - val_loss: 8.4740 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.5489 - accuracy: 1.0000 - val_loss: 8.5533 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.5178 - accuracy: 0.9951 - val_loss: 8.5479 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.4854 - accuracy: 1.0000 - val_loss: 8.6255 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.4576 - accuracy: 1.0000 - val_loss: 8.6077 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.4342 - accuracy: 1.0000 - val_loss: 8.6902 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.4108 - accuracy: 1.0000 - val_loss: 8.6378 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.3916 - accuracy: 1.0000 - val_loss: 8.6525 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.3750 - accuracy: 1.0000 - val_loss: 8.6896 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.3533 - accuracy: 0.9951 - val_loss: 8.7309 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.3367 - accuracy: 0.9951 - val_loss: 8.7943 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.3188 - accuracy: 1.0000 - val_loss: 8.8025 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.3049 - accuracy: 1.0000 - val_loss: 8.7841 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.2910 - accuracy: 1.0000 - val_loss: 8.8559 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.2709 - accuracy: 1.0000 - val_loss: 8.8338 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.2633 - accuracy: 1.0000 - val_loss: 8.8416 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.2540 - accuracy: 1.0000 - val_loss: 8.8630 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.2391 - accuracy: 1.0000 - val_loss: 8.8920 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.2313 - accuracy: 1.0000 - val_loss: 8.8784 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.2202 - accuracy: 1.0000 - val_loss: 8.9162 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 8.9769 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 9.0033 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.1993 - accuracy: 1.0000 - val_loss: 9.0538 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.1897 - accuracy: 1.0000 - val_loss: 9.0163 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 0.1816 - accuracy: 1.0000 - val_loss: 9.0170 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 0.1754 - accuracy: 1.0000 - val_loss: 9.0559 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.1704 - accuracy: 1.0000 - val_loss: 9.0704 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.1627 - accuracy: 1.0000 - val_loss: 9.0835 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 9.1215 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 1s 84ms/step - loss: 0.1515 - accuracy: 1.0000 - val_loss: 9.1025 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.1468 - accuracy: 1.0000 - val_loss: 9.0848 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.1440 - accuracy: 1.0000 - val_loss: 9.1581 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.1383 - accuracy: 1.0000 - val_loss: 9.1662 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.1345 - accuracy: 1.0000 - val_loss: 9.1588 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 9.1770 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 9.1779 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 9.1885 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.1186 - accuracy: 1.0000 - val_loss: 9.2484 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.1163 - accuracy: 1.0000 - val_loss: 9.2797 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.1131 - accuracy: 1.0000 - val_loss: 9.2868 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.1107 - accuracy: 1.0000 - val_loss: 9.2753 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.1077 - accuracy: 1.0000 - val_loss: 9.2928 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.1048 - accuracy: 1.0000 - val_loss: 9.2743 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.1014 - accuracy: 1.0000 - val_loss: 9.2887 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 9.3288 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 9.3628 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 9.3446 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 9.3639 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 9.3830 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0870 - accuracy: 1.0000 - val_loss: 9.3943 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0853 - accuracy: 1.0000 - val_loss: 9.4376 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 9.4401 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 9.4151 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 9.4329 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0774 - accuracy: 1.0000 - val_loss: 9.4569 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 9.4673 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 9.4619 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0731 - accuracy: 1.0000 - val_loss: 9.4803 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 9.5063 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 9.5053 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0685 - accuracy: 1.0000 - val_loss: 9.4984 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 9.5074 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.0647 - accuracy: 1.0000 - val_loss: 9.5316 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model 2 with pretrained embeddings\n",
        "RNNmod2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qxzMcHdjyP",
        "outputId": "d3b22555-dfa4-4b2a-e6a4-7216e01ce08f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9801 - accuracy: 0.0345 - val_loss: 8.9753 - val_accuracy: 0.0196\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 4.9799 - accuracy: 0.0345 - val_loss: 9.0581 - val_accuracy: 0.0196\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 4.9805 - accuracy: 0.0345 - val_loss: 8.9003 - val_accuracy: 0.0196\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 4.9794 - accuracy: 0.0345 - val_loss: 8.7193 - val_accuracy: 0.0196\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9804 - accuracy: 0.0345 - val_loss: 8.8007 - val_accuracy: 0.0196\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 4.9803 - accuracy: 0.0345 - val_loss: 8.7043 - val_accuracy: 0.0196\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 4.9805 - accuracy: 0.0345 - val_loss: 8.7526 - val_accuracy: 0.0196\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 4.9800 - accuracy: 0.0345 - val_loss: 8.8918 - val_accuracy: 0.0196\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.9803 - accuracy: 0.0345 - val_loss: 9.0037 - val_accuracy: 0.0196\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9790 - accuracy: 0.0345 - val_loss: 8.9599 - val_accuracy: 0.0196\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 4.9805 - accuracy: 0.0345 - val_loss: 9.0221 - val_accuracy: 0.0196\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 4.9808 - accuracy: 0.0345 - val_loss: 8.8180 - val_accuracy: 0.0196\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9795 - accuracy: 0.0345 - val_loss: 8.8152 - val_accuracy: 0.0196\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9797 - accuracy: 0.0345 - val_loss: 8.8300 - val_accuracy: 0.0196\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9806 - accuracy: 0.0345 - val_loss: 8.7731 - val_accuracy: 0.0196\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9787 - accuracy: 0.0345 - val_loss: 8.9645 - val_accuracy: 0.0196\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9797 - accuracy: 0.0345 - val_loss: 9.2536 - val_accuracy: 0.0196\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9799 - accuracy: 0.0345 - val_loss: 9.3151 - val_accuracy: 0.0196\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9806 - accuracy: 0.0345 - val_loss: 9.3128 - val_accuracy: 0.0196\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9792 - accuracy: 0.0345 - val_loss: 9.0416 - val_accuracy: 0.0196\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9792 - accuracy: 0.0345 - val_loss: 8.9528 - val_accuracy: 0.0196\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9792 - accuracy: 0.0345 - val_loss: 8.9513 - val_accuracy: 0.0196\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9809 - accuracy: 0.0345 - val_loss: 9.3754 - val_accuracy: 0.0196\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9814 - accuracy: 0.0345 - val_loss: 9.2765 - val_accuracy: 0.0196\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 4.9801 - accuracy: 0.0345 - val_loss: 9.3265 - val_accuracy: 0.0196\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9798 - accuracy: 0.0345 - val_loss: 9.1309 - val_accuracy: 0.0196\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 4.9795 - accuracy: 0.0345 - val_loss: 8.9960 - val_accuracy: 0.0196\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9784 - accuracy: 0.0345 - val_loss: 9.0126 - val_accuracy: 0.0196\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9790 - accuracy: 0.0345 - val_loss: 8.9898 - val_accuracy: 0.0196\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9774 - accuracy: 0.0345 - val_loss: 9.1306 - val_accuracy: 0.0196\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9785 - accuracy: 0.0345 - val_loss: 9.1818 - val_accuracy: 0.0196\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9787 - accuracy: 0.0345 - val_loss: 9.3084 - val_accuracy: 0.0196\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9799 - accuracy: 0.0345 - val_loss: 9.2627 - val_accuracy: 0.0196\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9784 - accuracy: 0.0345 - val_loss: 9.1800 - val_accuracy: 0.0196\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9796 - accuracy: 0.0345 - val_loss: 9.1128 - val_accuracy: 0.0196\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9780 - accuracy: 0.0345 - val_loss: 9.1978 - val_accuracy: 0.0196\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9794 - accuracy: 0.0345 - val_loss: 9.3866 - val_accuracy: 0.0196\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9793 - accuracy: 0.0345 - val_loss: 9.3034 - val_accuracy: 0.0196\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9781 - accuracy: 0.0345 - val_loss: 9.2200 - val_accuracy: 0.0196\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9778 - accuracy: 0.0345 - val_loss: 9.0137 - val_accuracy: 0.0196\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9776 - accuracy: 0.0345 - val_loss: 9.1059 - val_accuracy: 0.0196\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9777 - accuracy: 0.0345 - val_loss: 9.0452 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9775 - accuracy: 0.0296 - val_loss: 8.9885 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9783 - accuracy: 0.0296 - val_loss: 9.0933 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9782 - accuracy: 0.0296 - val_loss: 9.0799 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 4.9788 - accuracy: 0.0246 - val_loss: 9.0932 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 4.9785 - accuracy: 0.0296 - val_loss: 9.0164 - val_accuracy: 0.0196\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9795 - accuracy: 0.0345 - val_loss: 8.9097 - val_accuracy: 0.0196\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9786 - accuracy: 0.0345 - val_loss: 9.0099 - val_accuracy: 0.0196\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 4.9780 - accuracy: 0.0345 - val_loss: 9.2060 - val_accuracy: 0.0196\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 4.9779 - accuracy: 0.0345 - val_loss: 9.2378 - val_accuracy: 0.0196\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 4.9767 - accuracy: 0.0345 - val_loss: 9.0376 - val_accuracy: 0.0196\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 4.9783 - accuracy: 0.0345 - val_loss: 9.0061 - val_accuracy: 0.0196\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 4.9784 - accuracy: 0.0345 - val_loss: 9.0669 - val_accuracy: 0.0196\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 4.9783 - accuracy: 0.0345 - val_loss: 9.0305 - val_accuracy: 0.0196\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 4.9777 - accuracy: 0.0345 - val_loss: 9.1468 - val_accuracy: 0.0196\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9781 - accuracy: 0.0345 - val_loss: 9.2284 - val_accuracy: 0.0196\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9786 - accuracy: 0.0345 - val_loss: 9.3350 - val_accuracy: 0.0196\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9794 - accuracy: 0.0345 - val_loss: 9.2480 - val_accuracy: 0.0196\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9805 - accuracy: 0.0345 - val_loss: 9.4085 - val_accuracy: 0.0196\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9769 - accuracy: 0.0345 - val_loss: 9.1912 - val_accuracy: 0.0196\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9778 - accuracy: 0.0345 - val_loss: 9.1747 - val_accuracy: 0.0196\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9776 - accuracy: 0.0345 - val_loss: 9.1055 - val_accuracy: 0.0196\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9782 - accuracy: 0.0345 - val_loss: 9.0725 - val_accuracy: 0.0196\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9771 - accuracy: 0.0345 - val_loss: 9.3077 - val_accuracy: 0.0196\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9767 - accuracy: 0.0345 - val_loss: 9.3272 - val_accuracy: 0.0196\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9779 - accuracy: 0.0345 - val_loss: 9.2033 - val_accuracy: 0.0196\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9771 - accuracy: 0.0345 - val_loss: 9.3201 - val_accuracy: 0.0196\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9783 - accuracy: 0.0345 - val_loss: 9.4701 - val_accuracy: 0.0196\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 4.9789 - accuracy: 0.0345 - val_loss: 9.3990 - val_accuracy: 0.0196\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9770 - accuracy: 0.0345 - val_loss: 9.4106 - val_accuracy: 0.0196\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9770 - accuracy: 0.0345 - val_loss: 9.3011 - val_accuracy: 0.0196\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9782 - accuracy: 0.0345 - val_loss: 9.3943 - val_accuracy: 0.0196\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 4.9810 - accuracy: 0.0345 - val_loss: 9.1816 - val_accuracy: 0.0196\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9809 - accuracy: 0.0345 - val_loss: 9.6471 - val_accuracy: 0.0196\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9786 - accuracy: 0.0345 - val_loss: 9.5700 - val_accuracy: 0.0196\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9772 - accuracy: 0.0345 - val_loss: 9.3616 - val_accuracy: 0.0196\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 4.9762 - accuracy: 0.0345 - val_loss: 9.3916 - val_accuracy: 0.0196\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9776 - accuracy: 0.0345 - val_loss: 9.4862 - val_accuracy: 0.0196\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9775 - accuracy: 0.0296 - val_loss: 9.4519 - val_accuracy: 0.0196\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 4.9772 - accuracy: 0.0394 - val_loss: 9.3717 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 4.9767 - accuracy: 0.0296 - val_loss: 9.3056 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9770 - accuracy: 0.0296 - val_loss: 9.2213 - val_accuracy: 0.0196\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 4.9759 - accuracy: 0.0345 - val_loss: 9.2065 - val_accuracy: 0.0196\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 4.9770 - accuracy: 0.0345 - val_loss: 9.1851 - val_accuracy: 0.0196\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9778 - accuracy: 0.0345 - val_loss: 9.2749 - val_accuracy: 0.0196\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 4.9780 - accuracy: 0.0345 - val_loss: 9.3765 - val_accuracy: 0.0196\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9773 - accuracy: 0.0345 - val_loss: 9.2638 - val_accuracy: 0.0196\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9774 - accuracy: 0.0345 - val_loss: 9.3974 - val_accuracy: 0.0196\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 4.9769 - accuracy: 0.0345 - val_loss: 9.3043 - val_accuracy: 0.0196\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 4.9764 - accuracy: 0.0345 - val_loss: 9.1586 - val_accuracy: 0.0196\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 4.9762 - accuracy: 0.0345 - val_loss: 9.1605 - val_accuracy: 0.0196\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 4.9775 - accuracy: 0.0345 - val_loss: 9.3162 - val_accuracy: 0.0196\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 4.9768 - accuracy: 0.0345 - val_loss: 9.3804 - val_accuracy: 0.0196\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 4.9773 - accuracy: 0.0345 - val_loss: 9.3106 - val_accuracy: 0.0196\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9766 - accuracy: 0.0345 - val_loss: 9.3613 - val_accuracy: 0.0196\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 4.9767 - accuracy: 0.0345 - val_loss: 9.4664 - val_accuracy: 0.0196\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9780 - accuracy: 0.0345 - val_loss: 9.3057 - val_accuracy: 0.0196\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 4.9761 - accuracy: 0.0345 - val_loss: 9.2265 - val_accuracy: 0.0196\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 4.9767 - accuracy: 0.0345 - val_loss: 9.2263 - val_accuracy: 0.0196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION"
      ],
      "metadata": {
        "id": "k212atMMfup1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_model1 = RNNmod.history['val_loss'][-1]\n",
        "val_loss_model2 = RNNmod2.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss for Model 1: {val_loss_model1}\")\n",
        "print(f\"Validation Loss for Model 2: {val_loss_model2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6aYH-_YfpGU",
        "outputId": "7227accb-4576-422d-be15-98cc63fbcf7b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss for Model 1: 9.531639099121094\n",
            "Validation Loss for Model 2: 9.226271629333496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if val_loss_model1 < val_loss_model2:\n",
        "    best_model = model1\n",
        "    best_model_name = \"best_model1.h5\"\n",
        "else:\n",
        "    best_model = model2\n",
        "    best_model_name = \"best_model2.h5\"\n",
        "\n",
        "best_model.save(best_model_name)\n",
        "print(f\"Saved the best model as {best_model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-sXO7VpgD4R",
        "outputId": "bb0b835f-f507-40c3-b479-8fa854cdc4bc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the best model as best_model2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('best_model2.h5')\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, num_words=1):\n",
        "    \"\"\"\n",
        "    Predict the next set of words using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (keras.Model): The trained model.\n",
        "    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.\n",
        "    - text (str): The input text.\n",
        "    - num_words (int): The number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted words.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Tokenize and pad the text\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        # Convert the predicted word index to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "# Prompt the user for input\n",
        "user_input = input(\"Please type five words in Shona: \")\n",
        "\n",
        "# Predict the next words\n",
        "predicted_words = predict_next_words(model, tokenizer, user_input, num_words=3)\n",
        "print(f\"The next words might be: {predicted_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df66cVxzgGZr",
        "outputId": "1fb8e436-6ad4-4321-de5f-a49aa2872de3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please type five words in Shona: kana\n",
            "The next words might be: rondedzero rondedzero rondedzero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUAUaj3EgxMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}